{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f5db5ba-71b8-47ba-9fc8-49cf91cf1826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd4093f5-76df-4033-9d7d-0147c6b5d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using multi-GPU.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set your desired seed\n",
    "set_seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da677a08-1e15-4438-8dce-0a00c59b1d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notes Regarding versions:\n",
    "#3.6.13 python --> 3.8.10\n",
    "#0.10.0 gluonts --> 0.9.0\n",
    "#Version: 1.1.5 pandas --> 1.5.3\n",
    "#Version: 1.8.0 torch --> 2.1.5 --> 1.10.0 last version according to the issue answer in git\n",
    "#Version: 1.19.5 numpy --> 1.23.5\n",
    "#after this had issue regarding the distribution output importing changed the importing in the pts/dis_output line 34 to gluonts.torch.modules.distribution_output\n",
    "# and remove it from the importing cell.\n",
    "#encountered prefetch error handled it by downgrading torch version\n",
    "#to install gluonts 0.9.0 had to downgrade pip version to 24.0\n",
    "#!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40fd8239-f091-4bff-8d98-16d827813880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sys\n",
    "import pts.dataset\n",
    "#sys.path.append(r'C:\\Users\\usama\\Desktop\\Thesis\\pytorch-ts-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76a518ce-2371-4371-9883-00d7998e3b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.multivariate_grouper import MultivariateGrouper\n",
    "#import gluonts.torch.distributions.distribution_output\n",
    "from gluonts.dataset.repository.datasets import dataset_recipes, get_dataset\n",
    "from pts.model.tempflow import TempFlowEstimator\n",
    "from pts.model.transformer_tempflow import TransformerTempFlowEstimator\n",
    "from pts import Trainer\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.evaluation import MultivariateEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc874844-c6c8-4b7e-9999-a2bcaea187e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aef0292a-838d-48ac-b15d-aa6113f345e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetaData(freq='H', target=None, feat_static_cat=[CategoricalFeatureInfo(name='feat_static_cat_0', cardinality='137')], feat_static_real=[], feat_dynamic_real=[], feat_dynamic_cat=[], prediction_length=24)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = get_dataset(\"solar_nips\", regenerate=False)\n",
    "dataset.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "804df0a2-10ed-4d5d-b54e-bc2a4a507636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #for electricity data\n",
    "# from gluonts.dataset.common import ListDataset\n",
    "\n",
    "# # Define train and validation datasets separately\n",
    "# train_data = []\n",
    "# val_data = []\n",
    "\n",
    "# for i, entry in enumerate(dataset.train):\n",
    "#     series_length = len(entry['target'])\n",
    "#     train_size = int(0.8 * series_length)  # 80% of the actual series length\n",
    "#     val_size = series_length - train_size  # Remaining 20%\n",
    "    \n",
    "#     # Create train entry\n",
    "#     train_entry = entry.copy()\n",
    "#     train_entry['target'] = entry['target'][:train_size]  # First 80%\n",
    "#     train_data.append(train_entry)\n",
    "    \n",
    "#     # Create validation entry\n",
    "#     val_entry = entry.copy()\n",
    "#     val_entry['target'] = entry['target'][train_size:]  # Remaining 20%\n",
    "#     val_data.append(val_entry)\n",
    "    \n",
    "#     # Print verification\n",
    "#     print(f\"Series {i}: Train length = {train_size}, Validation length = {val_size}\")\n",
    "    \n",
    "#     # Assert the lengths are correct\n",
    "#     assert len(train_entry['target']) == train_size, f\"Train length mismatch for series {i}\"\n",
    "#     assert len(val_entry['target']) == val_size, f\"Validation length mismatch for series {i}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea087765-bce5-4675-b7ee-c6aea210151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grouper = MultivariateGrouper(max_target_dim=int(dataset.metadata.feat_static_cat[0].cardinality))\n",
    "test_grouper = MultivariateGrouper(num_test_dates=int(len(dataset.test)/len(dataset.train)), \n",
    "                                    max_target_dim=int(dataset.metadata.feat_static_cat[0].cardinality))\n",
    "# val_grouper = MultivariateGrouper(max_target_dim=int(dataset.metadata.feat_static_cat[0].cardinality))\n",
    "# test_grouper = MultivariateGrouper(num_test_dates=int(len(dataset.test)/len(train_data)), \n",
    "#                                    max_target_dim=int(dataset.metadata.feat_static_cat[0].cardinality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9178da4-47b9-46c1-b35e-0a48155888e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = ListDataset(train_data, freq=dataset.metadata.freq)\n",
    "# validation_dataset = ListDataset(val_data, freq=dataset.metadata.freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b32fcba-9be3-4cc3-9cdd-bae2b1375a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_train = train_grouper(train_dataset)\n",
    "dataset_train = train_grouper(dataset.train)\n",
    "dataset_test = test_grouper(dataset.test)\n",
    "# dataset_val = val_grouper(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c094beb-2cf9-4a67-91c5-e291af9a9553",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MultivariateEvaluator(quantiles=(np.arange(20)/20.0)[1:],\n",
    "                                  target_agg_funcs={'sum': np.sum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fbb4120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying learning_rate=1e-05, n_blocks=2\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'linear'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 43\u001b[0m\n\u001b[1;32m     24\u001b[0m estimator \u001b[38;5;241m=\u001b[39m TempFlowEstimator(\n\u001b[1;32m     25\u001b[0m     target_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mfeat_static_cat[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcardinality),\n\u001b[1;32m     26\u001b[0m     prediction_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     ),\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Train and evaluate the model\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m forecast_it, ts_it \u001b[38;5;241m=\u001b[39m make_evaluation_predictions(\n\u001b[1;32m     45\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mdataset_test, predictor\u001b[38;5;241m=\u001b[39mpredictor, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     47\u001b[0m forecasts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(forecast_it)\n",
      "File \u001b[0;32m~/Thesis/Thesis/pytorch-ts-master/pts/model/estimator.py:183\u001b[0m, in \u001b[0;36mPyTorchEstimator.train\u001b[0;34m(self, training_data, validation_data, num_workers, prefetch_factor, shuffle_buffer_length, cache_data, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    173\u001b[0m     training_data: Dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    182\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PyTorchPredictor:\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefetch_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefetch_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle_buffer_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle_buffer_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpredictor\n",
      "File \u001b[0;32m~/Thesis/Thesis/pytorch-ts-master/pts/model/estimator.py:105\u001b[0m, in \u001b[0;36mPyTorchEstimator.train_model\u001b[0;34m(self, training_data, validation_data, num_workers, prefetch_factor, shuffle_buffer_length, cache_data, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     93\u001b[0m     training_data: Dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TrainOutput:\n\u001b[1;32m    103\u001b[0m     transformation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_transformation()\n\u001b[0;32m--> 105\u001b[0m     trained_net \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_training_network\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     input_names \u001b[38;5;241m=\u001b[39m get_module_forward_input_names(trained_net)\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m env\u001b[38;5;241m.\u001b[39m_let(max_idle_transforms\u001b[38;5;241m=\u001b[39mmaybe_len(training_data) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[0;32m~/Thesis/Thesis/pytorch-ts-master/pts/model/tempflow/tempflow_estimator.py:189\u001b[0m, in \u001b[0;36mTempFlowEstimator.create_training_network\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_training_network\u001b[39m(\u001b[38;5;28mself\u001b[39m, device: torch\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TempFlowTrainingNetwork:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTempFlowTrainingNetwork\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_cells\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_cells\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcell_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcell_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprediction_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcardinality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_dimension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_dimension\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlags_seq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlags_seq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflow_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_hidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_hidden\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconditioning_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconditioning_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdequantize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdequantize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gluonts/core/component.py:344\u001b[0m, in \u001b[0;36mvalidated.<locals>.validator.<locals>.init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m__getnewargs_ex__ \u001b[38;5;241m=\u001b[39m validated_getnewargs_ex\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m \u001b[38;5;241m=\u001b[39m validated_repr\n\u001b[0;32m--> 344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Thesis/Thesis/pytorch-ts-master/pts/model/tempflow/tempflow_network.py:57\u001b[0m, in \u001b[0;36mTempFlowTrainingNetwork.__init__\u001b[0;34m(self, input_size, num_layers, num_cells, cell_type, history_length, context_length, prediction_length, dropout_rate, lags_seq, target_dim, conditioning_length, flow_type, n_blocks, hidden_size, n_hidden, dequantize, cardinality, embedding_dimension, scaling, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m rnn_cls \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m\"\u001b[39m: nn\u001b[38;5;241m.\u001b[39mLSTM, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGRU\u001b[39m\u001b[38;5;124m\"\u001b[39m: nn\u001b[38;5;241m.\u001b[39mGRU}[cell_type]\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn \u001b[38;5;241m=\u001b[39m rnn_cls(\n\u001b[1;32m     51\u001b[0m     input_size\u001b[38;5;241m=\u001b[39minput_size,\n\u001b[1;32m     52\u001b[0m     hidden_size\u001b[38;5;241m=\u001b[39mnum_cells,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     56\u001b[0m )\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m(\u001b[38;5;241m552\u001b[39m,\u001b[38;5;241m40\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#self.linear = nn.Linear(1484,40)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m flow_cls \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRealNVP\u001b[39m\u001b[38;5;124m\"\u001b[39m: RealNVP,\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAF\u001b[39m\u001b[38;5;124m\"\u001b[39m: MAF,\n\u001b[1;32m     62\u001b[0m }[flow_type]\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'linear'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.evaluation import MultivariateEvaluator\n",
    "\n",
    "# Define the grid for hyperparameter tuning\n",
    "learning_rates = [1e-5, 1e-4, 1e-3, 1e-2]  # Example learning rates\n",
    "n_blocks_list = [2, 3, 4, 5, 6]  # Number of blocks to try\n",
    "\n",
    "# Initialize variables to store the best results\n",
    "best_metric = float('inf')  # Smallest CRPS-Sum\n",
    "best_params = None\n",
    "\n",
    "# Multivariate evaluator for evaluation\n",
    "evaluator = MultivariateEvaluator(\n",
    "    quantiles=(np.arange(20) / 20.0)[1:], target_agg_funcs={'sum': np.sum}\n",
    ")\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for learning_rate in learning_rates:\n",
    "    for n_blocks in n_blocks_list:\n",
    "        print(f\"Trying learning_rate={learning_rate}, n_blocks={n_blocks}\")\n",
    "        \n",
    "        # Define the estimator\n",
    "        estimator = TempFlowEstimator(\n",
    "            target_dim=int(dataset.metadata.feat_static_cat[0].cardinality),\n",
    "            prediction_length=24,\n",
    "            cell_type='GRU',\n",
    "            input_size=552,\n",
    "            freq=dataset.metadata.freq,\n",
    "            scaling=True,\n",
    "            dequantize=True,\n",
    "            n_blocks=n_blocks,\n",
    "            trainer=Trainer(\n",
    "                device=device,\n",
    "                epochs=1,\n",
    "                learning_rate=learning_rate,\n",
    "                num_batches_per_epoch=100,\n",
    "                batch_size=64,\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        # Train and evaluate the model\n",
    "        predictor = estimator.train(dataset_train, num_workers=4)\n",
    "        forecast_it, ts_it = make_evaluation_predictions(\n",
    "            dataset=dataset_test, predictor=predictor, num_samples=100\n",
    "        )\n",
    "        forecasts = list(forecast_it)\n",
    "        targets = list(ts_it)\n",
    "        \n",
    "        # Evaluate the forecasts\n",
    "        agg_metric, _ = evaluator(targets, forecasts, num_series=len(dataset_test))\n",
    "        current_metric = agg_metric['m_sum_mean_wQuantileLoss']\n",
    "        \n",
    "        print(f\"CRPS-Sum for learning_rate={learning_rate}, n_blocks={n_blocks}: {current_metric}\")\n",
    "        \n",
    "        # Update the best parameters if the current metric is better\n",
    "        if current_metric < best_metric:\n",
    "            best_metric = current_metric\n",
    "            best_params = {\n",
    "                'learning_rate': learning_rate,\n",
    "                'n_blocks': n_blocks,\n",
    "            }\n",
    "\n",
    "# Print the best results\n",
    "print(f\"Best CRPS-Sum: {best_metric}\")\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4db642-2571-443d-9094-29754cac197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TempFlowEstimator(\n",
    "    target_dim=int(dataset.metadata.feat_static_cat[0].cardinality),\n",
    "    prediction_length=dataset.metadata.prediction_length,\n",
    "    cell_type='GRU',\n",
    "    input_size=1484,\n",
    "    freq=dataset.metadata.freq,\n",
    "    scaling=True,\n",
    "    dequantize=True,\n",
    "    n_blocks=4,\n",
    "    trainer=Trainer(device=device,\n",
    "                    epochs=45,\n",
    "                    learning_rate=0.013074418981914738,\n",
    "                    num_batches_per_epoch=100,\n",
    "                    batch_size=64)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aca563-3124-43a1-aecd-a53e94c86571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor = estimator.train(dataset_train, num_workers =4)\n",
    "forecast_it, ts_it = make_evaluation_predictions(dataset=dataset_test,\n",
    "                                             predictor=predictor,\n",
    "                                             num_samples=100)\n",
    "forecasts = list(forecast_it)\n",
    "targets = list(ts_it)\n",
    "\n",
    "agg_metric, _ = evaluator(targets, forecasts, num_series=len(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af94c0c-0131-4f27-837e-3d535e02a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlls = []\n",
    "for target, forecast in zip(targets, forecasts):\n",
    "    # Convert target to a numpy array if it is not already\n",
    "    target_array = np.asarray(target)  # Ensure the target is a numpy array\n",
    "\n",
    "    # Extract the target values for the prediction length\n",
    "    observed = target_array[-predictor.prediction_length:]  # Shape: (prediction_length,)\n",
    "\n",
    "    # Get forecast samples (shape: [num_samples, prediction_length, target_dim])\n",
    "    samples = forecast.samples  # Shape: (100, 24, 370)\n",
    "\n",
    "    # If multivariate, ensure dimensions match\n",
    "    if samples.ndim == 3:  # (num_samples, prediction_length, target_dim)\n",
    "        for dim in range(samples.shape[-1]):  # Iterate over dimensions\n",
    "            dim_samples = samples[:, :, dim]  # Shape: (100, 24)\n",
    "            dim_observed = observed[:, dim]   # Extract observed values for this dimension\n",
    "\n",
    "            # Estimate likelihoods for observed values under the empirical distribution\n",
    "            likelihoods = np.mean(\n",
    "                np.exp(-0.5 * ((dim_observed - dim_samples) ** 2)), axis=0\n",
    "            )  # Shape: (24,)\n",
    "            log_likelihoods = np.log(likelihoods + 1e-10)  # Add epsilon for numerical stability\n",
    "\n",
    "            # Compute NLL for this dimension\n",
    "            nll = -np.mean(log_likelihoods)\n",
    "            nlls.append(nll)\n",
    "    else:  # For univariate cases\n",
    "        likelihoods = np.mean(\n",
    "            np.exp(-0.5 * ((observed - samples) ** 2)), axis=0\n",
    "        )  # Shape: (24,)\n",
    "        log_likelihoods = np.log(likelihoods + 1e-10)  # Add epsilon for numerical stability\n",
    "\n",
    "        # Compute NLL for univariate case\n",
    "        nll = -np.mean(log_likelihoods)\n",
    "        nlls.append(nll)\n",
    "\n",
    "# Aggregate NLL across all series and dimensions\n",
    "average_nll = np.mean(nlls)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Negative Log-Likelihood (NLL): {average_nll}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469b3e41-c9d2-4b81-bda9-16ac60d778ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CRPS: {}\".format(agg_metric['mean_wQuantileLoss']))\n",
    "print(\"ND: {}\".format(agg_metric['ND']))\n",
    "print(\"NRMSE: {}\".format(agg_metric['NRMSE']))\n",
    "print(\"MSE: {}\".format(agg_metric['MSE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a81d38-605e-453e-bd63-c015d218eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CRPS-Sum: {}\".format(agg_metric['m_sum_mean_wQuantileLoss']))\n",
    "print(\"ND-Sum: {}\".format(agg_metric['m_sum_ND']))\n",
    "print(\"NRMSE-Sum: {}\".format(agg_metric['m_sum_NRMSE']))\n",
    "print(\"MSE-Sum: {}\".format(agg_metric['m_sum_MSE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d6ed5b-d972-4138-8271-fcda1a507ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3280245e-2c0f-4fee-b377-720057bb175d",
   "metadata": {},
   "source": [
    "### Tunning for Electricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a66c67d-583a-4750-8ec7-a5505f17a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest values for hyperparameters\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-7, 1e-1)\n",
    "    #batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    #epochs = trial.suggest_int(\"epochs\", 25, 75)\n",
    "    n_blocks = trial.suggest_int(\"n_blocks\", 2, 8)\n",
    "    \n",
    "    # Initialize the Trainer with suggested parameters\n",
    "    trainer = Trainer(\n",
    "        device=device,\n",
    "        epochs=10,\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=64,\n",
    "        num_batches_per_epoch=100\n",
    "    )\n",
    "    \n",
    "    # Initialize the Estimator with suggested parameters\n",
    "    estimator = TempFlowEstimator(\n",
    "        target_dim=int(dataset.metadata.feat_static_cat[0].cardinality),\n",
    "        prediction_length=dataset.metadata.prediction_length,\n",
    "        cell_type='GRU',\n",
    "        input_size=1484,\n",
    "        freq=dataset.metadata.freq,\n",
    "        scaling=True,\n",
    "        dequantize=True,\n",
    "        n_blocks=n_blocks,\n",
    "        trainer=trainer\n",
    "    )\n",
    "    \n",
    "    # Train the model and get the predictor\n",
    "    predictor = estimator.train(dataset_train, dataset_val, num_workers=4)\n",
    "    \n",
    "    # Return the validation loss for the current trial (assuming Trainer has avg_val_loss attribute)\n",
    "    return trainer.avg_val_loss\n",
    "\n",
    "# Run the Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=1)\n",
    "\n",
    "# Output best parameters and validation loss\n",
    "print(\"Best parameters found:\", study.best_params)\n",
    "print(\"Best validation loss:\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c99fccc-717f-479c-8281-c57857cb9caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a482945f-a238-4598-8b94-ffbccc9d824c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602e3fc2-1140-49b6-9f93-3f17b5f67207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6856ac-291e-425b-aae7-cdce77505f11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
