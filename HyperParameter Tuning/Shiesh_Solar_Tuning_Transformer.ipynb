{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f5db5ba-71b8-47ba-9fc8-49cf91cf1826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd4093f5-76df-4033-9d7d-0147c6b5d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sys\n",
    "import pts.dataset\n",
    "#sys.path.append(r'C:\\Users\\usama\\Desktop\\Thesis\\pytorch-ts-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da677a08-1e15-4438-8dce-0a00c59b1d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notes Regarding versions:\n",
    "#3.6.13 python --> 3.8.10\n",
    "#0.10.0 gluonts --> 0.9.0\n",
    "#Version: 1.1.5 pandas --> 1.5.3\n",
    "#Version: 1.8.0 torch --> 2.1.5 --> 1.10.0 last version according to the issue answer in git\n",
    "#Version: 1.19.5 numpy --> 1.23.5\n",
    "#after this had issue regarding the distribution output importing changed the importing in the pts/dis_output line 34 to gluonts.torch.modules.distribution_output\n",
    "# and remove it from the importing cell.\n",
    "#encountered prefetch error handled it by downgrading torch version\n",
    "#to install gluonts 0.9.0 had to downgrade pip version to 24.0\n",
    "#!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40fd8239-f091-4bff-8d98-16d827813880",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using multi-GPU.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set your desired seed\n",
    "set_seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76a518ce-2371-4371-9883-00d7998e3b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.multivariate_grouper import MultivariateGrouper\n",
    "#import gluonts.torch.distributions.distribution_output\n",
    "from gluonts.dataset.repository.datasets import dataset_recipes, get_dataset\n",
    "from pts.model.tempflow import TempFlowEstimator\n",
    "from pts.model.transformer_tempflow import TransformerTempFlowEstimator\n",
    "from pts import Trainer\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.evaluation import MultivariateEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc874844-c6c8-4b7e-9999-a2bcaea187e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aef0292a-838d-48ac-b15d-aa6113f345e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetaData(freq='30min', target=None, feat_static_cat=[CategoricalFeatureInfo(name='feat_static_cat_0', cardinality='1214')], feat_static_real=[], feat_dynamic_real=[], feat_dynamic_cat=[], prediction_length=24)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = get_dataset(\"taxi_30min\", regenerate=False)\n",
    "dataset.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea087765-bce5-4675-b7ee-c6aea210151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grouper = MultivariateGrouper(max_target_dim=int(dataset.metadata.feat_static_cat[0].cardinality))\n",
    "test_grouper = MultivariateGrouper(num_test_dates=int(len(dataset.test)/len(dataset.train)), \n",
    "                                    max_target_dim=int(dataset.metadata.feat_static_cat[0].cardinality))\n",
    "# val_grouper = MultivariateGrouper(max_target_dim=int(dataset.metadata.feat_static_cat[0].cardinality))\n",
    "# test_grouper = MultivariateGrouper(num_test_dates=int(len(dataset.test)/len(train_data)), \n",
    "#                                    max_target_dim=int(dataset.metadata.feat_static_cat[0].cardinality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b32fcba-9be3-4cc3-9cdd-bae2b1375a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_train = train_grouper(train_dataset)\n",
    "dataset_train = train_grouper(dataset.train)\n",
    "dataset_test = test_grouper(dataset.test)\n",
    "# dataset_val = val_grouper(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c094beb-2cf9-4a67-91c5-e291af9a9553",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MultivariateEvaluator(quantiles=(np.arange(20)/20.0)[1:],\n",
    "                                  target_agg_funcs={'sum': np.sum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fbb4120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying learning_rate=1e-08, num_heads=2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43018a287a95441187c9926e79a625c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 47\u001b[0m\n\u001b[1;32m     27\u001b[0m estimator \u001b[38;5;241m=\u001b[39m TransformerTempFlowEstimator(\n\u001b[1;32m     28\u001b[0m     d_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     29\u001b[0m     num_heads\u001b[38;5;241m=\u001b[39mnum_heads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Train and evaluate the model\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m forecast_it, ts_it \u001b[38;5;241m=\u001b[39m make_evaluation_predictions(\n\u001b[1;32m     49\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mdataset_test, predictor\u001b[38;5;241m=\u001b[39mpredictor, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     51\u001b[0m forecasts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(forecast_it)\n",
      "File \u001b[0;32m~/Thesis/Thesis/pytorch-ts-master/pts/model/estimator.py:183\u001b[0m, in \u001b[0;36mPyTorchEstimator.train\u001b[0;34m(self, training_data, validation_data, num_workers, prefetch_factor, shuffle_buffer_length, cache_data, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    173\u001b[0m     training_data: Dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    182\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PyTorchPredictor:\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefetch_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefetch_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle_buffer_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle_buffer_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpredictor\n",
      "File \u001b[0;32m~/Thesis/Thesis/pytorch-ts-master/pts/model/estimator.py:153\u001b[0m, in \u001b[0;36mPyTorchEstimator.train_model\u001b[0;34m(self, training_data, validation_data, num_workers, prefetch_factor, shuffle_buffer_length, cache_data, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     validation_iter_dataset \u001b[38;5;241m=\u001b[39m TransformedIterableDataset(\n\u001b[1;32m    136\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mvalidation_data,\n\u001b[1;32m    137\u001b[0m         transform\u001b[38;5;241m=\u001b[39mtransformation\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m         cache_data\u001b[38;5;241m=\u001b[39mcache_data,\n\u001b[1;32m    142\u001b[0m     )\n\u001b[1;32m    143\u001b[0m     validation_data_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m    144\u001b[0m         validation_iter_dataset,\n\u001b[1;32m    145\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrained_net\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m TrainOutput(\n\u001b[1;32m    160\u001b[0m     transformation\u001b[38;5;241m=\u001b[39mtransformation,\n\u001b[1;32m    161\u001b[0m     trained_net\u001b[38;5;241m=\u001b[39mtrained_net,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m     ),\n\u001b[1;32m    165\u001b[0m )\n",
      "File \u001b[0;32m~/Thesis/Thesis/pytorch-ts-master/pts/trainer.py:86\u001b[0m, in \u001b[0;36mTrainer.__call__\u001b[0;34m(self, net, train_iter, validation_iter)\u001b[0m\n\u001b[1;32m     77\u001b[0m avg_epoch_loss \u001b[38;5;241m=\u001b[39m cumm_epoch_loss \u001b[38;5;241m/\u001b[39m batch_no\n\u001b[1;32m     78\u001b[0m it\u001b[38;5;241m.\u001b[39mset_postfix(\n\u001b[1;32m     79\u001b[0m     {\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_no\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m     refresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     84\u001b[0m )\n\u001b[0;32m---> 86\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_gradient \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(net\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_gradient)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.evaluation import MultivariateEvaluator\n",
    "\n",
    "# Define the grid for hyperparameter tuning\n",
    "learning_rates = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]  # Example learning rates\n",
    "num_head_list = [2, 3, 4, 5, 6, 7, 8]  # Number of Heads to try\n",
    "#num_d_model = [16, 32, 64, 128]\n",
    "# n_blocks_list = [2, 3, 4, 5, 6]  # Number of blocks to try\n",
    "\n",
    "# Initialize variables to store the best results\n",
    "best_metric = float('inf')  # Smallest CRPS-Sum\n",
    "best_params = None\n",
    "\n",
    "# Multivariate evaluator for evaluation\n",
    "evaluator = MultivariateEvaluator(\n",
    "    quantiles=(np.arange(20) / 20.0)[1:], target_agg_funcs={'sum': np.sum}\n",
    ")\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for learning_rate in learning_rates:\n",
    "    for num_heads in num_head_list:\n",
    "    \n",
    "        print(f\"Trying learning_rate={learning_rate}, num_heads={num_heads}\")\n",
    "        \n",
    "        # Define the estimator\n",
    "        estimator = TransformerTempFlowEstimator(\n",
    "            d_model=16,\n",
    "            num_heads=num_heads,\n",
    "            input_size=7290,\n",
    "            target_dim=int(dataset.metadata.feat_static_cat[0].cardinality),\n",
    "            prediction_length=dataset.metadata.prediction_length,\n",
    "            context_length=dataset.metadata.prediction_length * 4,\n",
    "            flow_type='MAF',\n",
    "            dequantize=True,\n",
    "            freq=dataset.metadata.freq,\n",
    "            trainer=Trainer(\n",
    "                device=device,\n",
    "                epochs=40,\n",
    "                learning_rate=learning_rate,\n",
    "                num_batches_per_epoch=100,\n",
    "                batch_size=64,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Train and evaluate the model\n",
    "        predictor = estimator.train(dataset_train, num_workers=4)\n",
    "        forecast_it, ts_it = make_evaluation_predictions(\n",
    "            dataset=dataset_test, predictor=predictor, num_samples=100\n",
    "        )\n",
    "        forecasts = list(forecast_it)\n",
    "        targets = list(ts_it)\n",
    "        \n",
    "        # Evaluate the forecasts\n",
    "        agg_metric, _ = evaluator(targets, forecasts, num_series=len(dataset_test))\n",
    "        current_metric = agg_metric['m_sum_mean_wQuantileLoss']\n",
    "        \n",
    "        print(f\"CRPS-Sum for learning_rate={learning_rate}, num_heads={num_heads}: {current_metric}\")\n",
    "        \n",
    "        # Update the best parameters if the current metric is better\n",
    "        if current_metric < best_metric:\n",
    "            best_metric = current_metric\n",
    "            best_params = {\n",
    "                'learning_rate': learning_rate,\n",
    "                'num_heads': num_heads,\n",
    "            }\n",
    "\n",
    "# Print the best results\n",
    "print(f\"Best CRPS-Sum: {best_metric}\")\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d6a063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
